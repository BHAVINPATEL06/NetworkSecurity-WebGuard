# ğŸ›¡ï¸ Network Security (WebGuard) ML Project ğŸ›¡ï¸

## ğŸ“‹ Overview
This project is a complete end-to-end machine learning pipeline for network security analysis built during Krish Naik's Machine Learning Bootcamp. It uses advanced ML techniques to detect and classify network security threats from network data.

## ğŸ› ï¸ Tech Stack
- **ğŸ Programming Language**: Python 3.10
- **ğŸš€ Web Framework**: FastAPI
- **ğŸ—„ï¸ Database**: MongoDB
- **ğŸ“Š ML Libraries**:
  - scikit-learn
  - pandas
  - numpy
- **âš™ï¸ MLOps Tools**:
  - MLflow (for experiment tracking)
  - DagsHub (for versioning and collaboration)
- **â˜ï¸ Cloud Services**: AWS S3 (for artifact and model storage)
- **ğŸ³ Container**: Docker
- **ğŸ“š Other Libraries**:
  - pymongo
  - python-dotenv
  - dill (for object serialization)

## ğŸ“ Project Structure

### ğŸ“‚ Root Directory
- `app.py`: FastAPI application for serving predictions
- `main.py`: Entry point to run the complete pipeline
- `push_data.py`: Script to push data from CSV to MongoDB
- `setup.py`: Package installation and setup file
- `requirements.txt`: Project dependencies
- `Dockerfile`: Docker containerization configuration

### ğŸ“¦ Core Directories
- `network_security/`: Main package containing all modules
  - `components/`: Core components of the ML pipeline
    - `data_ingestion.py`: ğŸ“¥ Data collection from MongoDB
    - `data_validation.py`: âœ… Validate the quality of ingested data
    - `data_transformation.py`: âš—ï¸ Feature engineering and preprocessing
    - `model_trainer.py`: ğŸ§  Model training and selection
  - `entity/`: ğŸ“ Data classes for config and artifacts
  - `pipeline/`: ğŸ”„ Pipeline orchestration code
  - `utils/`: ğŸ”§ Utility functions for ML and operations
  - `cloud/`: â˜ï¸ Cloud integration code (S3 syncing)
  - `constants/`: ğŸ“‹ Project constants
  - `exception/`: âš ï¸ Custom exception handling
  - `logging/`: ğŸ“ Logging configuration

### ğŸ’¾ Data and Artifacts
- `Network_Data/`: ğŸ“Š Raw data directory
- `Artifacts/`: ğŸ“¦ Stores artifacts from each pipeline run
- `final_model/`: ğŸ† Contains the final trained model and preprocessor
- `logs/`: ğŸ“œ Application logs
- `templates/`: ğŸ–¥ï¸ HTML templates for web interface

## ğŸš€ How to Run the Project

### ğŸ“‹ Prerequisites
1. ğŸ Python 3.10 or higher
2. ğŸ—„ï¸ MongoDB instance
3. â˜ï¸ AWS account (optional, for S3 storage)

### ğŸ”§ Environment Setup
1. ğŸ“¥ Clone the repository
   ```bash
   git clone <repository-url>
   cd Network-Security-Machine-Learning-Project
   ```

2. ğŸ”‘ Create a `.env` file in the root directory with the following:
   ```
   MONGO_URI=<your-mongodb-connection-string>
   AWS_ACCESS_KEY_ID=<your-aws-access-key>
   AWS_SECRET_ACCESS_KEY=<your-aws-secret-key>
   ```

3. ğŸ“¦ Install dependencies
   ```bash
   pip install -r requirements.txt
   ```
   
   Or install using setup.py:
   ```bash
   pip install -e .
   ```

### ğŸ“¤ Data Ingestion
1. Push data to MongoDB:
   ```bash
   python push_data.py
   ```

### ğŸ§  Model Training
Run the complete training pipeline:
```bash
python main.py
```

This will:
1. ğŸ“¥ Ingest data from MongoDB
2. âœ… Validate the data
3. ğŸ”„ Transform and prepare features
4. ğŸ§ª Train multiple models and select the best one
5. ğŸ’¾ Save the model artifacts
6. â˜ï¸ Upload artifacts to S3 (if configured)

### ğŸš€ Running the Prediction Service
Start the FastAPI service:
```bash
python app.py
```

The API will be available at http://localhost:8000 with the following endpoints:
- ğŸ“š `/`: Documentation (redirects to `/docs`)
- ğŸ§  `/train`: Endpoint to trigger model training
- ğŸ”® `/predict`: Endpoint to make predictions (requires file upload)

### ğŸ³ Using Docker
Build and run with Docker:
```bash
docker build -t network-security-ml .
docker run -p 8000:8000 -d network-security-ml
```

## ğŸ“Š MLflow Integration
The project integrates with MLflow and DagsHub for experiment tracking:

```python
# Access the experiment tracking interface
# MLflow UI will be available through DagsHub
```

## ğŸ”„ Project Workflow
1. ğŸ“¤ Data is loaded from a CSV file into MongoDB
2. ğŸ“¥ Data is ingested from MongoDB
3. âœ… Data is validated against a schema
4. âš—ï¸ Features are transformed and preprocessed
5. ğŸ§ª Multiple ML models are trained and evaluated:
   - ğŸŒ³ Random Forest
   - ğŸŒ² Decision Tree
   - ğŸ“ˆ Gradient Boosting
   - ğŸ“Š Logistic Regression
   - ğŸ”‹ AdaBoost
6. ğŸ† The best model is selected based on performance metrics
7. ğŸ’¾ The model is saved for deployment
8. ğŸš€ The model is served via a FastAPI application

